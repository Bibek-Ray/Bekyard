<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Convolution Neural Network (CNN)</title>
  <link href="https://fonts.googleapis.com/css2?family=Fira+Sans:wght@400;600;700&family=JetBrains+Mono&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <a href="../index.html" class="back-home">← Back to Home</a>
  <section>
    <div class="cover-image">
      <img src="./cnn assets/img2.png" class="cover-art" alt="Neural Network Cover Art">
    </div>
    <h1>Convolution Neural Network (CNN)</h1>
    <p class="meta">Author: Bibek Ray &nbsp;&nbsp;|&nbsp;&nbsp; Date: 12th May 2025</p>
    
    <p>I hope you are here after reading my blog on ANNs. If you haven’t read it yet, I plead with you to read it once, it doesn’t matter where you read (<em>P.S. - Read mine</em>). I said that because CNNs are just another layer on top of ANNs.<br/>
    CNNs are specifically designed to handle image-related data, since the ANN is not that great with pixels. Before getting into the technical jargon, let me give you an easy-to-understand top-down view of how a CNN works.<br/>
    Let’s first imagine how we humans understand images. How do we <em>“recognize”</em> them? Their shape, color, or segmenting objects by edges. These help us identify what we’re looking at.<br/>
    In the same way, CNNs work. They try to detect edges and sometimes even color them too. How does it detect? Well, that’s the juicy part.</p>
    
    <h2>Let’s get an input dataset first…</h2>
    <p>We need some data first. And guess what our sample data will be? An image of a cat! It would be <em>purrfect</em> for our intuition.</p>
    <p>Look at this innocent guy:</p>
    <img src="cnn assets/Cat face.jpg" alt="Cat Image">
    
    <p>We both find it cute, but what about the computer?<br/>
      It doesn’t understand images. However good-looking the photograph is, you give it high Resolution, 8K, it just won’t get it. It only understands <em>numbers</em>. So we need to think of some way to convert this cat into numbers.<br/>
      By the way, on a side note, I recall one very famous line I once read in a YouTube comment section. It said,<br/>
      <blockquote><em>“Anything that can be represented in numbers can be used to train an ML model”.</em></blockquote><br/>
      Quite insightful.<br/><br/>
      Back to what we were discussing, converting a cat into a number.<br/>
      First, we need to understand what an image is. An image is, in very simple words, a 3-dimensional matrix— height, width, and the RGB color channel. Height and Width are variables, but color channel can’t go beyond 3, because no one has discovered any more primary colors other than Red, Green, and Blue…<em>yet</em>.<br/>
      This image data can be converted to gray scale to remove the RGB dimension. That will also benefit in reducing the computational resources, but it comes with a cost of “Loss of Information”.<br/>
      Once converted to grayscale, you can represent the pixels with their actual value. The pixel values <strong>range</strong> from 0 to 255: 0 is black, 255 is white. We can and do normalize it to a range of 0-1 for making the calculation simple.<br/>
      Check the image below. Zoom in, and see how I converted it into a matrix. This is what a machine will be able to see or read.</p>
    
    <img src="cnn assets/grayscale_matrix_with_values.png" alt="Grayscale Matrix">
    <p class="meta">This is just a 32x32 matrix for illustration. Don’t judge my cat.</p>

    <p>This one is a 32x32 matrix and thus, looks horrible. It’s just for illustration purposes, actual images are way more detailed to pass as much information as possible.<br/>
      Let’s normalize the image first. Check the updated matrix,</p>
    
    <img src="cnn assets/normalized_pixel_matrix.png" alt="Normalized Matrix">
    <p class="meta">See? Smaller and simpler numbers to work with.</p>
    <p>Notice that the pixel values are much smaller and easier to work with now. This reduces the chances of getting into heavy numbers during Matrix calculations, which will be pretty cumbersome for the machine to work with.<br/>
      Right after this we can do an optional step, called <strong><span>Padding</span></strong>.<br/>
      Padding helps preserve edge information. Think of it like this: a pixel in the middle of the image usually has <strong>8 neighboring pixels</strong> for comparison. But the pixel at the top-left corner (like position (0, 0)) only has <strong>3 neighbors</strong>. That imbalance affects how well the kernel detects textures and edges near the image borders.<br/>
      This becomes important <strong>if the image has relevant features near the edges</strong>. But in many cases (like our adorable cat image), the subject is usually <strong>centered</strong>, and the edges contain background or less useful data.<br/>
      That’s why padding is optional, but it’s still good to know it exists — especially when you want to preserve image size or detect border features better.<br/></p>
      <img src="./cnn assets/Padding Illustration.png" alt="Padding Illustration">
      <p class="meta">This is how padding is added</p>
      <p>Before we move on to the next part, I’ll be explaining you what a <strong><span>Kernel</span></strong> is. It’s important to understand it first.</p>
    
    <h2>The Kernel (aka Filter)</h2>
    <p>The Kernel is a small matrix that goes through the entire image matrix and highlights the important pixel in it. Think of it as picking up a coin from a box of scrap.<br/>
      I’ve provided an illustration below to show how it goes over the image matrix.</p>
    <img src="cnn assets/Kernel Slide.svg" alt="Kernel over image">
    <p class="meta">Kernel Matrix sliding over the Image Matrix</p>
    
    <p>Now, how does the kernel detect features? By using simple math.<br/>
      We take the chunk of the image matrix over which the kernel box is hovering and multiply it's values with the kernel matrix's and add the values of the resultant matrix. This is done for the entire image and the resultant individual values are stored in a new matrix called <strong><span>Feature Map</span></strong>.<br/>
      So, let's say we have a 6 x 6 image matrix and a kernel of size 3 x 3 with stride = 1, then our feature map will be of size 4 x 4.<br/>
      The following formula helps in getting the Feature Map size,</p>
    <blockquote>$\text{Output Size} = \frac{(N - F)}{S} + 1$</blockquote>
    <p>Where,<br/>
      <ul>
        <li>N = Input size (height/width)</li>
        <li>F = Filter (kernel) size</li>
        <li>S = Stride</li>
      </ul>
      <small class="meta">(Assuming no padding is used)</small>
    </p>
    <p>For our Normalized Cat Image matrix, the computed Feature Map looks like this,</p>
    <div class="two-column-image">
      <div>
        <img src="cnn assets/sobel_gradient_output.png" alt="Cat face feature map without values">
        <p class="meta">Feature Map without values</p>
      </div>
      <div>
        <img src="cnn assets/sobel_gradient_with_values.png" alt="Cat face feature map">
        <p class="meta">Cat's Face Feature Map using Sobel Kernel</p>
      </div>
    </div>
    <p>I’ve used a pre-defined Kernel called “Sobel Kernel”(two kernels — one for vertical and one for horizontal edge detection). It’s used for edge detection specifically. But, in traditional CNNs, the Kernel matrix is part of the model training.</p>
    <h2>Pooling</h2>
    <p>Once we get the feature map, we further shrink it to make training easier and faster. This shrinking process is called <strong><span>Pooling</span></strong>.<br/>
      We can either take the average of all the pixel values, to preserve the information for all the pixels into one single value, or we can select the maximum value out of all the matrix values. The former one is called <strong>mean-pooling</strong> while the latter is the famous one, called <strong>max-pooling</strong>.</p>
    <img src="cnn assets/Max-Pool.svg" alt="Kernel Sliding">
    <p class="meta">Max-pooling in action</p>  
    <p>Pooling shrinks the feature map significantly—Like around 9 values getting replaced by 1 if we consider the pixels selected from the central region of the feature map.<br/>
      The pooled feature map in our cat example looks like this,</p>
      <img src="cnn assets/Pooled_feature_map_with_values.png" alt="Pooled Feature Map">
      <p class="meta">Pooled Feature Map of Traumatised Cat</p>
      <p>Now this is what will be our final matrix that will be flattened and fed into the ANN for training.<br/><br/>
        Hell of a ride.<br/><br/>
        BTW, a flattened matrix is a 2-dimensional matrix converted to a 1-dimensional matrix. Kinda like a <strong>list</strong> in Python or <strong>array</strong> in <em>C++</em> or <em>Java</em>—you get it.</p>
    <h2>The ANN Layer</h2>
      <p>I have explained this one in my <a href="https://bekyard.vercel.app/posts/ann.html" target="_blank">ANN</a> blog, and almost everything from there remains the same, except the activation function in the last hidden layer.<br/>
        For Image Classification, we use the Sigmoid Activation Function instead of ReLU. Sometimes we use the Softmax Activation Function instead, especially when classifying more than two objects.<br/>
        For our present case, let’s assume we are classifying between dogs and cats.<br/>        
        The Sigmoid Activation Function is this,</p>
        <blockquote>$\sigma(x) = \frac{1}{1 + e^{-x}}$</blockquote>
        <p>It looks like this,</p>
        <iframe src="https://www.desmos.com/calculator/87kp4h1xfw?embed" width="800" height="400" style="border: 1px solid #ccc" frameborder=0></iframe>
        <p>The sigmoid function outputs values between 0 and 1 — which can be interpreted as the model’s confidence. For example, a value of 0.92 means the model is 92% sure the image is a cat (or whatever class it's predicting).</p>
      <h2>The Loss Function</h2>
      <p>Ah, here comes the actual learning part — for the model and us both.<br/>
      The ANN currently will spit trash values in the name of a confidence score for the animal being cat. We need to fix that, moreover, the Loss function needs to fix that.<br/>
        First, let me show you the famous Binary Cross Entropy Loss Function <em>(Cool name right?)</em>,</p>
        <blockquote>$\mathcal{L}_{\text{BCE}} = -\left[ y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right]$</blockquote>
      <p>Where,<br/>

          $y = \text{The actual label}$<br/>
          
          $\hat{y} = \text{The predicted label}$<br/>
          
          Below I have shown a graph, have a look at that.</p>
        <iframe src="https://www.desmos.com/calculator/5d7jydiyrw?embed" width="300" height="300" style="border: 1px solid #ccc" frameborder=0></iframe>
        <p>The black line represents the BCE Loss function’s variation when the actual label is 1 and the predicted value moves between 0 and 1.</p>
        <p>When the prediction is close to 1, the loss is low — the model is confident and right. But as the predicted value starts getting closer to 0, the loss increases rapidly. That sharp rise is the model being <strong>penalized</strong> for being confidently wrong.</p>
        <p>This is how the loss function <em>teaches</em> the model: by punishing it harder the more wrong and overconfident it is.</p>
        <p>The red line represents the exact opposite case, where actual is 0 and the predicted value fluctuates as usual. Same penalization, different direction.</p>
        <h2>Backpropagation's Comeback</h2>
        <p>Finally the <em>OG</em> Backpropagation comes to play. We trace back through the entire CNN and ANN pipeline fixing the weights and biases to reduce our loss.<br/>
        We have discussed the formula in our <a href="https://bekyard.vercel.app/posts/ann.html">ANN</a><em>(P.S.- Read it!)</em> blog, but let me reiterate it once again,</p>
        <blockquote>$w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial \mathcal{L}}{\partial w}$</blockquote>
        <p>You might be wondering, <em>"Hey, the loss function doesn't even have $w$ in it, how will it get partially differentiated by it?"</em><br/>
           If you actually thought that—<strong>Good Observation Kiddo</strong><br/>
           See, $\hat{y}$ is actually a function of weights and biases:<br/>
           <blockquote>$\hat{y}=f(w,b)$</blockquote><br/>
           Thus, it can be partially differentiated w.r.t $w$ and $b$ both.</p>
          <h2>What's next?</h2>
          <p>Nothing.<br/>
          <strong>What!?</strong> Really, there nothing else!<br/>
          It's all the same stuff that happened in ANN. Same old, iterations after iterations until the loss reduces to a certain threshold value. But yeah, this time a bit longer this time, since we are not only updating the ANN's weights and biases but the Kernel matrix's values too.
          Remember that this time our input layer for the ANN is getting values from the flattened Pooled Matrix, which in itself is coming from the stacked Feature Maps that we got from each Kernel Matrix operation on the Image Matrix.</p>
          <p>I tried to keep this blog less mathematical and more pictorial because CNN in itself is designed for images, and so it would have been quite ironic.</p>
        <h2>Read more...</h2>
        <p>If you're still here, then congratulations—you are now 32x32% cooler.<br/>
          Check out my blog on <a href="https://bekyard.vercel.app/posts/ann.html" target="_blank">ANN</a> if you missed it.<br/>
          Follow me on <a href="https://www.linkedin.com/in/bibek-ray-061727220/" target="_blank">LinkedIn</a>.<br/>
          And if you liked the humor here, do drop a feedback(On LinkedIn). Or a cat emoji. That works too.
        </p>   
    <script>
      MathJax = {
        tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
        svg: { fontCache: 'global' }
      };
      </script>
</section>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>
